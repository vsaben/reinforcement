---
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Ex. 2.1**: $0.5 + 0.5*0.5 = 0.75$

**Ex. 2.2**: Random action is definite only where a non-optimal action is selected.

```{r table, echo = F, warning=F, message=F}

library(knitr)

df <- data.frame("Time" = 0:5, 
                 "Q(1)" = c(0, -1, -1, -1, -1, -1), 
                 "Q(2)" = c(0, 0, 1, -0.5, 0.33, 0.33), 
                 "Q(3)" = c(0, 0, 0, 0, 0, 0),
                 "Q(4)" = c(0, 0, 0, 0, 0, 0), 
                 "Random Action" = c("Possible", "Possible", "Possible", "Definite", "Definite", ""))

kable(df)
```
  
**Ex. 2.3**: $\epsilon = 0.01$. It will have a higher probability ($1 - 0.01 = 0.99$ vs. $1 - 0.1 = 0.9$) of selecting the best action relative to $\epsilon = 0.1$. The extent of improvement above a greedy policy is dependent on the greedy method's initial performance.  

**Ex. 2.4**: 
$$
\begin{aligned}
Q_{n+1} &= Q_n + \alpha_n[R_n - Q_n] \\
         &= \alpha_n R_n + (1 - \alpha_n) Q_n \\
         &= \alpha_n R_n + (1 - \alpha_n) \times (\alpha_{n-1} R_{n-1} + (1 - \alpha_{n-1}) Q_{n-1}) \\
         &= \alpha_n R_n + (1 - \alpha_n)\alpha_{n-1} R_{n-1} + (1 - \alpha_n)(1 - \alpha_{n-1})Q_{n-1} \\
         &= \alpha_n R_n + (1 - \alpha_n)\alpha_{n-1} R_{n-1} + (1 - \alpha_n)(1 - \alpha_{n-1})\alpha_{n-2} R_{n-2} + \dots + \alpha_1\prod_{i=1}^n(1 - \alpha_i) R_1 + \prod_{i=1}^n(1 - \alpha_i)Q_1\\
         &= \prod_{i=1}^n(1 - \alpha_i)Q_1 + \sum_{i=1}^n\alpha_i\prod_{i+1, i<n}^n(1 - \alpha_i)R_i
\end{aligned}
$$

**Ex. 2.5**:
\
```{r policy, message=F, warning=F, echo = F}

library(Matrix)
library(ggplot2)

# Note: argmax takes first index corresponding to maximum. Favour learning to earlier states, 
#  but states are randomly ordered in this context. 

policy <- function(alpha = F, nrun = 2000, nbandit = 10, nstep = 10000, epsilon = 0.1){
  
  "Computes average reward and optimal action % for a 
  nbandit problem, aggregated across nrun.
  
  :param alpha: incremental step (F) or alpha (constant) 
  :param nrun: number of runs
  :param nbandit: number of bandits
  :param nstep: number of steps
  
  :result average reward: 
  :result optimal acrion %:
  "

  # Initialised variables  
  
  q <- matrix(0, nrow = nrun, ncol = nbandit)
  Q <- matrix(0, nrow = nrun, ncol = nbandit)
  
  AveReward <- rep(0, nstep)
  OptAction <- rep(0, nstep)
  
  alpha = if(alpha){rep(alpha, nstep)} else {rep(1, nstep)/(1:nstep)}
  
  # Update Q
  
  for(t in 1:nstep){
    
    # 1: Update q
    
    dq <- matrix(rnorm(nrun*nbandit, mean = 0, sd = 0.1),
                 nrow = nrun, 
                 ncol = nbandit)
    
    q <- q + dq
    
    # 2: Select action and update Q
    
    eps <- runif(nrun) >= epsilon
    col_ind <- apply(Q, 1, which.max) * eps + sample(1:nbandit, nrun, replace = T) * (1 - eps)
    opt_act <- apply(q, 1, which.max) == col_ind
    active <- as.matrix(sparseMatrix(i = 1:nrun, j = col_ind, dims = c(nrun, nbandit)))
    
    R <- q
    Q <- Q + alpha[t] * (R - Q) * active * eps
    
    # 3: Update aggregators
  
    AveReward[t] <- sum(R * active) / nrun
    OptAction[t] <- sum(opt_act) / nrun
    
    print(c(t/nstep, AveReward[t], OptAction[t]))
    
  }
  
  return(cbind(AveReward, OptAction))
  
}

# Parameters

NRUN <- 2000
NBANDIT <- 10
NSTEP <- 10000
EPSILON <- 0.1

```

```{r run 2.5, eval = F, echo = F}

# Calculations

Incremental <- policy(alpha = F, NRUN, NBANDIT, NSTEP, EPSILON)
Constant <- policy(alpha = 0.1, NRUN, NBANDIT, NSTEP, EPSILON)

```


```{r load 2.5, echo = F}

load('Constant.RData')
load('Incremental.RData')

```


```{r graph 2.5 A, message=F, warning=F, echo = F, out.width="80%", out.height="80%"}

# Graph A: Average Reward

AveReward = cbind.data.frame(Incremental[, 1], Constant[, 1])
names(AveReward) = c('I', 'C')

ggplot(data = AveReward, aes(1:NSTEP)) + 
  geom_line(aes(y=I), colour='blue') + 
  geom_line(aes(y=C), colour='red') + 
  xlab('Steps') +
  ylab('Average Reward') + 
  ggtitle(expression(paste('Average Reward for an Incremental and Constant ',
                           alpha, 
                           ', Across 2000 Runs', sep = ""))) +
  annotate('text', x = 2500, y = 2.5, label = "alpha==1/n", parse = T, colour = 'blue') +
  annotate('text', x = 2500, y = 6.25, label = "alpha==0.1", parse = T, colour = 'red') + 
  theme_minimal()

```

```{r graph 2.5 B, message=F, warning=F, echo = F, out.width="80%", out.height="80%"}

# Graph B: Optimal Action

OptAction = cbind.data.frame(Incremental[, 2], Constant[, 2])
names(OptAction) = c('I', 'C')

ggplot(data = OptAction, aes(1:NSTEP)) + 
  geom_line(aes(y=I), colour='blue') + 
  geom_line(aes(y=C), colour='red') + 
  xlab('Steps') +
  ylab('% Optimal Action') +
  ggtitle(expression(paste('Optimal Action % for Incremental and Constant ', 
                            alpha, 
                            ', Across 2000 Runs', sep = ""))) +
  annotate('text', x = 1000, y = 0.2, label = "alpha==1/n", parse = T, colour = 'blue') +
  annotate('text', x = 1000, y = 0.375, label = "alpha==0.1", parse = T, colour = 'red') + 
  theme_minimal()

```

**Ex 2.6**: Expect worse performance, on average, as the optimistic strategy encourages more exploration during earlier steps. The oscillations occur immediately after all states have been tested (10 steps). The optimistic strategy causes all states to be reached quicker, given the large initial Q-values of untouched states. At the 11th step and soon thereafter, more optimal action selection occurs given the models breadth of experience. This optimistic effect subsequently wears off and learning resembles a realistic scenario.

**Ex 2.7**: No initial bias exists as the $Q_1$ factor equates to 0.

$$
\begin{aligned}
Q_{n+1} &= Q_n + \beta_n[R_n - Q_n] \\
        &= \prod_{i=1}^n(1 - \beta_i)Q_1 + \sum_{i=1}^n\beta_i\prod_{i+1, i<n}^n(1 - \beta_i)R_i \\
        \\
\prod_{i=1}^n(1 - \beta_i) &= \prod_{i=1}^n(1 - \alpha/\theta_i) \\
                           &= (1 - \alpha/\theta_1) \prod_{i=2}^n(1 - \alpha/\theta_i) \qquad \text{where} \quad \theta_1 = \theta_0 + \alpha(1 - \theta_0) = \alpha \\
                           &= (1 - \alpha/\alpha) \prod_{i=2}^n(1 - \alpha/\theta_i) \\
                           &= 0
\end{aligned}
$$

**Ex. 2.8**: UCB (with c = 2) explores all bandits within 10 steps as $a$ is maximising if $N_t(a) = 0$. The 11th step is more informed relative to a slower $\epsilon$-greedy explorer (which is unlikely to have seen all states) thereby resulting in a reward spike. A drop in relative performance is subsequently experienced as the level of exploration is still high $Q_0(i) = 2\sqrt{\frac{ln(t)}{1}} \quad \text{where} \quad t > 10$ (e.g. $t=11, Q_{11}(i) + 3.1$) prior to an action's second selection, thus decreasing greedy selections. Exploration tapers off as $t$ increases.  

**Ex 2.9**: 

$$\pi_t(a) = \frac{e^{H_t(a)}}{e^{H_t(a)} + e^{H_t(b)}} = \frac{1}{1 + e^{H_t(b) - H_t(a)}} = \frac{1}{1 + e^{-(H_t(a) - H_t(b))}}$$
**Ex 2.10**:

$A_1 = a: E[R] = 0.5*(0.1 + 0.9) = 0.5$ \
$A_2 = a: E[R] = 0.5*(0.2 + 0.8) = 0.5$ \

Best expectation of success: $0.5$ \
Strategy: Choose randomly \

Case A: $E[R] = 0.2$ \
Case B: $E[R] = 0.9$  
\
Strategy: Depending on the case, select the action that has the highest expected value (in effect a 4 state problem).

**Ex 2.11**:

```{r params 2.11, echo = F}

# Parameters

NRUN <- 2000
NBANDIT <- 10
NSTEP <- 200000
EPS <- 2**seq(-7, -2)
NEPS <- length(EPS)

```

```{r run 2.11, eval = F, message=F, warning=F, echo = F}

library(foreach)
library(doSNOW)

# Calculations 

cl <- makeCluster(7)
registerDoSNOW(cl)

Epsilon <- foreach(i=1:NEPS, 
                   .combine = cbind, .packages = "Matrix", .verbose = T) %dopar% {
  
  policy(alpha = 0.1, nrun = NRUN, nbandit = NBANDIT, nstep = NSTEP, epsilon = EPS[i])

}

stopCluster(cl)

```


```{r load 2.11, echo = F}

load("Epsilon.RData")

```


```{r graph 2.11 A, message=F, warning=F, echo = F, out.width="80%", out.height="80%"}

library(reshape2)
library(tidyr)  
library(dplyr)

# Graph A: Average Reward 

AReward <- cbind.data.frame(1:NSTEP, Epsilon[, seq(1, by = 2, length = NEPS)]) 
names(AReward) <- c('Steps', EPS)
AveReward <- melt(AReward, id.vars = 'Steps')
names(AveReward) <- c('Steps', 'Epsilon', 'value')

ggplot(data = AveReward, aes(Steps, value, col = Epsilon)) + 
  geom_line() + 
  ggtitle(expression(paste('Average Reward for Increasing ', 
                            epsilon, 
                            '-Greedy Strategies, Across 2000 Runs', sep = ""))) +
  xlab('Steps') +
  ylab('Average Reward') + 
  theme_minimal()

```

```{r graph 2.11 B, message=F, warning=F, echo = F, out.width="80%", out.height="80%"}

# Graph B: Optimal Action %

OAct <- cbind.data.frame(1:NSTEP, Epsilon[, seq(2, by = 2, length = NEPS)])
names(OAct) <- c('Steps', EPS)
OptAct <- melt(OAct, id.vars = 'Steps')
names(OptAct) <- c('Steps', 'Epsilon', 'value')

ggplot(data = OptAct, aes(Steps, value, col = Epsilon)) + 
  geom_line() + 
  ggtitle(expression(paste('Optimal Action % for Increasing ', 
                           epsilon,
                           '-Greedy Strategies, Across 2000 Runs', sep = ""))) +
  xlab('Steps') +
  ylab('Optimal Action %') +
  theme_minimal()

```

```{r graph 2.11 C, message=F, warning=F, echo = F, out.width="80%", out.height="80%"}

# Graph C: Average Reward over Last 100 000 steps

AveRewardLast <- AveReward %>% 
  filter(Steps > 100000) %>% 
  group_by(Epsilon) %>% 
  summarise(reward = mean(value))

ggplot(data = AveRewardLast, aes(as.numeric(levels(Epsilon)), reward)) + 
  geom_line() + 
  geom_point() +
  xlab(expression(epsilon)) +
  ylab('Average reward over the last 100 000 steps') + 
  ggtitle(expression(paste('Average Reward over the Last 100k Steps for Different ', 
                           epsilon,
                           '-Greedy Strategies', sep = ""))) + 
  theme_minimal()

```



